{
  "version": "1",
  "metadata": {
    "marimo_version": "0.19.7"
  },
  "cells": [
    {
      "id": "MJUe",
      "code_hash": "e0fa690ff09918e1f8878373fbcd6bde",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"iowa-liquor-sales-analysis-comprehensive-overview\">IOWA LIQUOR SALES ANALYSIS - COMPREHENSIVE OVERVIEW</h2>\n<span class=\"paragraph\">This Marimo notebook demonstrates basic data engineering capabilities for handling large datasets efficiently:</span>\n<span class=\"paragraph\"><strong>Key Technical Achievements:</strong></span>\n<ol>\n<li><strong>Memory efficient data processing</strong> - Successfully analyzes a 6.99 gb dataset (26.8 million rows) with code that can be reproduced on a standard laptop using Polars' lazy evaluation.</li>\n<li><strong>Lazy Evaluation</strong> - Uses Polars' \"LazyFrames\" to build queries that process data in chunks, rather than loading entire dataset into memory, demonstrating abilities to use company data that would usually overwhelm standard workstations.</li>\n<li><strong>End-to-End Basic Analytics Project</strong> - Showcases data ingestion, data type optimization, category mapping, descriptive statistics, and interactive visuals while maintaining low memory requirements.\n<strong>Technical Stack:</strong> Python, Polars (Lazy Evaluation), Marimo (reactive notebooks), Plotly (visualizations)</li>\n</ol>\n<hr />\n<span class=\"paragraph\">This .py marimo file provides an overview of my abilities to use Python that can benefit a company in the following ways:</span>\n<ol>\n<li>It showcases my analytics abilities in Python to read large files onto a computer that would usually crash an underpowered workstation. This is relevant for companies collecting time-series data (data over time) or other types of large datasets that can quickly overwhelm a personal computer or an underpowered desktop.</li>\n<li>This analysis also shows that I can use Polars to understand a data frame's structure, and provide basic descriptive statistics/visualizations. I use Polars for this analysis due to its ability to perform analyses without needing to load the entire dataset to ram, such as how Pandas requires.</li>\n</ol>\n<span class=\"paragraph\">The data for this analysis covers 12 years of liquor sales data from retail establishments across the US state of Iowa (2012 - 2023). It includes records of all liquor sales from licensed retailers, including:</span>\n<ul>\n<li>Individual transactions from various retail stores</li>\n<li>Product details (categories, vendors, item descriptions)</li>\n<li>Pricing information (wholesale costs and retail prices)</li>\n<li>Geographic data (counties, cities, ZIP codes, and store locations)</li>\n<li>Sales metrics (bottles sold, revenue, and volume)</li>\n<li>Geographic Coverage: Multiple counties and cities across Iowa</li>\n<li>Product Diversity: Various liquor categories and vendors</li>\n</ul>\n<hr />\n<h2 id=\"analysis-structure\">ANALYSIS STRUCTURE</h2>\n<h3 id=\"task-10-ingest-the-data\">Task 1.0: Ingest the data</h3>\n<ul>\n<li><strong>Find the size of the Data File</strong></li>\n<li>\n<ol>\n<li>Found the size of the data file the standard way, loading the data frame in its entirety onto ram (Do not run if your computer has less than 8 gb of ram. Restart kernel if computer has less than 12 gb of ram.)</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>Found the size of the data file using an estimate of a sample of the rows, reducing RAM usage and being within 1 mb of the actual data frame size</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"task-20-21-data-preparation-optimization\">Task 2.0-2.1: Data Preparation &amp; Optimization</h3>\n<ul>\n<li><strong>Data Type Optimization</strong>: Converted columns to optimal types</li>\n<li><strong>Feature Engineering</strong>: Add new analytical columns such as temporal features (e.g., Quarter) and business metrics as required for my analyses</li>\n</ul>\n<h3 id=\"task-30-applying-labels-to-liquor-categories\">Task 3.0: Applying Labels to Liquor Categories</h3>\n<ul>\n<li>Converted 104 unique liquor descriptions from transactions into 13 major categories of liquor sold</li>\n<li>Example: Scotch, Bourbon, Whiskey, Whisky, and Whiskie all falling under major category of \"Whiskey\"</li>\n</ul>\n<h3 id=\"task-40-basic-descriptive-analyses\">Task 4.0: Basic Descriptive Analyses</h3>\n<ol>\n<li><strong>Analysis 1: Total Revenue and Volume Summary</strong> - High-level overview of entire dataset</li>\n<li><strong>Analysis 2: Top 10 Product Categories by Revenue</strong> - Category-level market shares</li>\n<li><strong>Analysis 3: Quarterly Sales Trends</strong> - Time-series visualization of revenue across all quarters</li>\n</ol>\n<h3 id=\"task-50-product-geographic-analysis-visualizations\">Task 5.0: Product &amp; Geographic Analysis Visualizations</h3>\n<span class=\"paragraph\"><strong>Product Analysis:</strong></span>\n<ul>\n<li><strong>Visualization 1: Top 20 Products by Revenue</strong> - Identify core revenue drivers</li>\n<li><strong>Visualization 2: Top 20 Products by Volume</strong> - Shows highest-volume items and consumer preferences</li>\n</ul>\n<span class=\"paragraph\"><strong>Geographic Analysis:</strong></span>\n<ul>\n<li><strong>Visualization 3: Top 15 Counties by Revenue:</strong> County-level market concentration</li>\n<li><strong>Visualization 4: Top 20 Cities by Sales:</strong>  City-level performance patterns</li>\n<li><strong>Visualization 6: Top 20 Cities by Sales Efficiency:</strong>  Average sales per transaction (standardized for transaction volume)</li>\n</ul>\n<span class=\"paragraph\"><strong>Temporal &amp; Behavioral Analysis:</strong></span>\n<ul>\n<li><strong>Visualization 5: Weekday vs Weekend Sales Comparison (Standardized):</strong>  Apples-to-apples comparison accounting for 5 weekdays vs 2 weekend days per week, showing average daily sales patterns</li>\n</ul>\n<hr />\n<h2 id=\"data-source\">DATA SOURCE</h2>\n<span class=\"paragraph\">Iowa Alcoholic Beverages Division\n<a href=\"https://data.iowa.gov/\" rel=\"noopener noreferrer\" target=\"_blank\">https://data.iowa.gov/</a></span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "vblA",
      "code_hash": "4b7b259d703997fd4e6d72e7618e0cf1",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"task-10-ingest-data\">Task 1.0: Ingest data</h2>\n<span class=\"paragraph\">For this first task, I ingest the data using lazy evaluation, in case of the data file being too large to load locally onto my computer. I only do what is specifically required to understand the size of the dataframe at this point. Below, I show my results and subsequently the code I used to obtain my results.</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "bkHC",
      "code_hash": "7c28725ee02048b6a43147593ad2c358",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"task-10-finding-the-size-of-the-data-file\">Task 1.0: Finding the size of the data file.</h3>\n<span class=\"paragraph\"><strong>Answer</strong>: It has 26,840,921 rows and 29 columns.</span>\n<span class=\"paragraph\">I next converted the size of the data file into megabytes and found the following solution.</span>\n<span class=\"paragraph\"><strong>Answer</strong>: My DataFrame size was 8,120.09 megabytes</span>\n<span class=\"paragraph\">Because finding the megabyte size required collecting the whole DataFrame, restarting the kernel is necessary to remove the added RAM usage. I ran another method that estimates the size of the data frame without collecting the whole file, and I found the following result.</span>\n<span class=\"paragraph\"><strong>Estimate Answer</strong>: The estimated DataFrame size had the same number of rows, columns, and was 8,119.94 megabytes, within 1 megabyte of the actual DataFrame size, without requiring to load the DataFrame into memory. This saves approximately 8 gigabytes of memory when loading the DataFrame.</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "SFPL",
      "code_hash": "2925d8e87ced37545e94316bd086d914",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"task-20-examine-and-clean-the-data\">Task 2.0: Examine and clean the data</h2>\n<span class=\"paragraph\">Now that I understand the structure of the data, I go through a variety of Python Polars commands below to examine the number of nulls, data types, and form an understanding of what to do next to make the data frame more optimal for performance and to be able to perform certain analyses.</span>\n<ul>\n<li>I first provide my results for my analysis of null values in the data, followed by the code I used to obtain these results.</li>\n<li>Next, I show my results for understanding the data types for each column of the data frame, followed by the code I used to understand problems with the original data types. These set up the next section of my project, which shows how I fixed the problems identified in this section.</li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "BYtC",
      "code_hash": "fff6a91b39a08d79dcb170f0fa1fc5e4",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"data-quality-assessment-null-values\">Data Quality Assessment - Null Values</h3>\n<span class=\"paragraph\">From the null count analysis, I identified the following patterns:</span>\n<span class=\"paragraph\">**Geographic Columns:</span>\n<ul>\n<li>\n<span class=\"paragraph\">These had high null counts - ~2.5M rows:</span>\n<ul>\n<li>Iowa ZIP Code Tabulation Areas: 2,506,295 nulls</li>\n<li>Iowa Watershed Sub-Basins: 2,506,295 nulls</li>\n<li>Iowa Watersheds: 2,506,295 nulls</li>\n<li>County Boundaries of Iowa: 2,506,295 nulls</li>\n<li>Store Location: 2,497,233 nulls</li>\n</ul>\n</li>\n</ul>\n<span class=\"paragraph\"><strong>Interpretation:</strong> These columns appear to show geographic details about each transaction. The high null count (~9% of data) suggests that these rows lack precise coordinate data or couldn't be matched to geographic boundaries. Since my analysis focuses on sales, product perofrmance, and patterns over time, these nulls are okay and don't provide me any major issues to deal with (I delete these columns later).</span>\n<span class=\"paragraph\"><strong>Critical Columns:</strong></span>\n<ul>\n<li>Date: 0 nulls</li>\n<li>Sale (Dollars): 0 nulls</li>\n<li>Bottles Sold: 0 nulls</li>\n<li>State Bottle Cost: 10 nulls</li>\n<li>Item Description: 3 nulls</li>\n</ul>\n<span class=\"paragraph\"><strong>Interpretation:</strong> The core transaction data is nearly complete (99.99%+). The 10 rows missing State Bottle Cost and 3 missing Item Description represent negligible data loss if removed.</span>\n<span class=\"paragraph\"><strong>Overall Data Quality:</strong> 81% of rows (21.7M out of 26.8M) have zero null values, indicating high overall data quality.</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "Hstk",
      "code_hash": "1d0a46fffa546b593065fa3dd3ec774e",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"data-type-issues-i-identified-below-this-markdown-cell\">Data Type Issues I Identified (below this markdown cell)</h3>\n<span class=\"paragraph\"><strong>Problem 1: Date is stored as String</strong></span>\n<ul>\n<li>Issue: Because of this, I cannot perform any temporal analyses or extract year/month/quarter</li>\n<li>Evidence: <code>df.schema</code> and 'df.describe' shows <code>('Date', String)</code></li>\n<li>Solution: Change to DateTime type</li>\n</ul>\n<span class=\"paragraph\"><strong>Problem 2: Zip Code causing errors</strong></span>\n<ul>\n<li>Issue: This column contains values like '712-2' that can't be parsed as integers</li>\n<li>Evidence: Initial data load from beginning of .py file failed with <code>ComputeError: could not parse '712-2' as dtype 'i64'</code> until I stated to ignore errors.</li>\n<li>Solution: Force Zip Code entries to be String type</li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "qnkX",
      "code_hash": "3dd15a2a338229541d9ea755e367baf3",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"task-21-data-type-optimization-and-feature-engineering\">Task 2.1: Data Type Optimization and Feature Engineering</h2>\n<h3 id=\"data-type-optimization\">Data Type Optimization</h3>\n<span class=\"paragraph\">As shown in the last section, the raw dataset has several columns with suboptimal data types. I made changes to the dataframe and recorded the changes in the table below.</span>\n<table>\n<thead>\n<tr>\n<th>Column Name</th>\n<th>Original Type</th>\n<th>Optimized Type</th>\n<th>Reasoning</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Date</td>\n<td>String</td>\n<td>DateTime</td>\n<td>Required to generate columns to make an analysis over time</td>\n</tr>\n<tr>\n<td>Zip Code</td>\n<td>Int64</td>\n<td>String (Utf8)</td>\n<td>Zip Codes are categorical, not numeric values. This prevents leading zeros and inappropriate mathematical operations</td>\n</tr>\n<tr>\n<td>Pack</td>\n<td>Int64</td>\n<td>Int8</td>\n<td>Pack sizes ranged from 1-24, and Int8 uses less memory for same values</td>\n</tr>\n<tr>\n<td>Bottle Volume (ml)</td>\n<td>Int64</td>\n<td>Int32</td>\n<td>Bottle volumes are less than 32,767, so Int32 is sufficient and uses much less memory</td>\n</tr>\n<tr>\n<td>Bottles Sold</td>\n<td>Int64</td>\n<td>Int16</td>\n<td>Transactions do not go high enough to justify above Int16</td>\n</tr>\n<tr>\n<td>Vendor Number</td>\n<td>Int64</td>\n<td>Int16</td>\n<td>Vendor IDs were small from examples in glimpse, and Int16 is sufficient while using less memory</td>\n</tr>\n<tr>\n<td>Store Number</td>\n<td>Int64</td>\n<td>Int16</td>\n<td>Store numbers were small from glimpse values, and Int16 is sufficient</td>\n</tr>\n<tr>\n<td>Date</td>\n<td>String</td>\n<td>DateTime</td>\n<td>Required to generate columns to make an analysis over time</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"columns-removed\">Columns Removed</h3>\n<table>\n<thead>\n<tr>\n<th>Column Name</th>\n<th>Reason for Removal</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Iowa ZIP Code Tabulation Areas</td>\n<td>Column not needed for future analysis &amp; has significant null values (~2.5 million)</td>\n</tr>\n<tr>\n<td>Iowa Watershed Sub-Basins (HUC 08)</td>\n<td>Column not needed for future analysis &amp; has significant null values (~2.5 million)</td>\n</tr>\n<tr>\n<td>Iowa Watersheds (HUC 10)</td>\n<td>Column not needed for future analysis &amp; has significant null values (~2.5 million)</td>\n</tr>\n<tr>\n<td>County Boundaries of Iowa</td>\n<td>Column not needed for future analysis &amp; has significant null values (~2.5 million)</td>\n</tr>\n<tr>\n<td>US Counties</td>\n<td>Column not needed for future analysis &amp; has significant null values (~2.5 million)</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"feature-engineering-for-additional-analysis\">Feature Engineering for Additional Analysis</h3>\n<span class=\"paragraph\">I added the following columns:</span>\n<table>\n<thead>\n<tr>\n<th>New Column</th>\n<th>Type</th>\n<th>Purpose</th>\n<th>Calculation</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>year</td>\n<td>Int32</td>\n<td>Looking at annual trends &amp; filtering by year</td>\n<td>built from the Date column using pl.col('Date').dt.year()</td>\n</tr>\n<tr>\n<td>month</td>\n<td>Int32</td>\n<td>Looking at monthly seasonality and trends</td>\n<td>built from the Date column using pl.col('Date').dt.month()</td>\n</tr>\n<tr>\n<td>quarter</td>\n<td>Int32</td>\n<td>Quarterly sales analysis (future problem)</td>\n<td>Built from the Date column using pl.col('Date').dt.quarter()</td>\n</tr>\n<tr>\n<td>day_of_week</td>\n<td>Int8</td>\n<td>Looking at patterns over the day of the week</td>\n<td>Built from the Date column using pl.col('Date').dt.weekday()</td>\n</tr>\n<tr>\n<td>is_weekend</td>\n<td>Boolean</td>\n<td>Weekday vs weekend sales comparison</td>\n<td>Built from the Date column &amp; calculated as 'day_of_week &gt;=5'</td>\n</tr>\n<tr>\n<td>price_per_bottle</td>\n<td>Float64</td>\n<td>To show how much each bottle costs in an individual transaction</td>\n<td>Calculated as 'Sale (Dollars) / Bottles Sold'</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"other-changes-to-the-data\">Other changes to the data</h3>\n<ul>\n<li>10 rows removed where cells for transaction-relevant columns (Date, Sale(Dollars), or Bottles Sold) were missing</li>\n<li>Retained all 2.5 million rows with geographic missing values since we are not assessing geographic variables in analysis</li>\n<li>Overwrote original Zip Code column with a Zip Code column that cast integer values as strings</li>\n<li>Increased the column count from 29 columns to 35 columns (added 6 columns and replaced one original)</li>\n</ul>\n<span class=\"paragraph\">-</span>\n<h3 id=\"rationale-for-optimization\">Rationale for Optimization</h3>\n<span class=\"paragraph\">-<strong>DateTime conversion allows more analyses</strong>: You cannot do analyses over time filtering with string date values.</span>\n<span class=\"paragraph\">-<strong>Quarter column is needed for Task 5.3</strong>: The Sales by Quarter visualization needed quarterly data, which is only possible after I created a new column that aggregated date values by quarter.</span>\n<span class=\"paragraph\">-<strong>Weekend dummy allows for further analyses</strong>: Visualization 5 requires comparing weekday vs weekend sales patterns. Having the dummy using a boolean flag makes filtering simple.</span>\n<span class=\"paragraph\">-<strong>Price per bottle standardizes the efficiency comparison</strong>: Visualization 6 compares cities by sales efficiency, and this metric prevents repeated division operations.</span>\n<span class=\"paragraph\">-<strong>Preserving geographic missing data prevents unnecessary data loss</strong>: While not optimal that much of this data is null, removing 2.5 million rows would eliminate 9% of our sample of transactions for analyses not using these columns with null values.</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "Vxnm",
      "code_hash": "1f57ee520b17e542faa437e66ce8bdae",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"task-30-apply-major-category-labels\">Task 3.0 Apply Major Category labels</h2>\n<span class=\"paragraph\">Using the industry-standard classification below, I created a column called \"Major Category\" and ensured all applicable rows of the dataset were placed in one of the following categories. If a row didn't match any category, I labeled it \"UNCATEGORIZED\"</span>\n<span class=\"paragraph\"><strong>WHISKEY</strong></span>\n<span class=\"paragraph\"><strong>VODKA</strong></span>\n<span class=\"paragraph\"><strong>RUM</strong></span>\n<span class=\"paragraph\"><strong>TEQUILA &amp; MEZCAL</strong></span>\n<span class=\"paragraph\"><strong>GIN</strong></span>\n<span class=\"paragraph\"><strong>RANDY &amp; COGNAC</strong></span>\n<span class=\"paragraph\"><strong>SCHNAPPS</strong></span>\n<span class=\"paragraph\"><strong>LIQUEURS &amp; CORDIALS</strong></span>\n<ul>\n<li>AMARETTO</li>\n<li>CORDIALS</li>\n<li>LIQUEURS</li>\n<li>ANISETTE</li>\n<li>CREME</li>\n<li>RYE</li>\n<li>TRIPLE SEC</li>\n</ul>\n<span class=\"paragraph\"><strong>SPECIALTY &amp; OTHER SPIRITS</strong></span>\n<ul>\n<li>AMERICAN ALCOHOL</li>\n<li>AMERICAN DISTILLED SPIRITS SPECIALTY</li>\n<li>DISTILLED SPIRITS SPECIALTY</li>\n<li>IMPORTED DISTILLED SPIRITS SPECIALTY</li>\n<li>NEUTRAL GRAIN SPIRITS</li>\n<li>NEUTRAL GRAIN SPIRITS FLAVORED</li>\n</ul>\n<span class=\"paragraph\"><strong>READY-TO-DRINK</strong></span>\n<ul>\n<li>AMERICAN COCKTAILS</li>\n<li>COCKTAILS/RTD</li>\n</ul>\n<span class=\"paragraph\"><strong>CRAFT/LOCAL</strong></span>\n<ul>\n<li>IOWA DISTILLERIES</li>\n</ul>\n<span class=\"paragraph\"><strong>ADMINISTRATIVE/NON-PRODUCT</strong></span>\n<ul>\n<li>DECANTERS &amp; SPECIALTY PACKAGES</li>\n<li>DELISTED / SPECIAL ORDER ITEMS</li>\n<li>DELISTED ITEMS</li>\n<li>HIGH PROOF BEER - AMERICAN</li>\n<li>HOLIDAY VAP</li>\n<li>SPECIAL ORDER ITEMS</li>\n<li>TEMPORARY &amp; SPECIALTY PACKAGES</li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "ecfG",
      "code_hash": "4eb6efc761fa8615f196b4837d6b96e3",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"task-40-basic-descriptive-of-the-data\">Task 4.0 Basic Descriptive of the Data</h2>\n<h3 id=\"41-analysis-1-total-revenue-and-volume-summary\">4.1 Analysis 1: Total Revenue and Volume Summary</h3>\n<span class=\"paragraph\">Provides high-level overview of the entire dataset</span>\n<ul>\n<li>Total Sales (Dollars)</li>\n<li>Total Bottles Sold</li>\n<li>Total Transactions</li>\n<li>Average Sale per Transaction</li>\n<li>Average Bottles per Transaction</li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "ZBYS",
      "code_hash": "80f4ab93fef149c89e905aa0a8b19bb3",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"42-analysis-2-top-10-product-categories-by-revenue\">4.2 Analysis 2: Top 10 Product Categories by Revenue</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "nHfw",
      "code_hash": "a6f2d116a11a21587815ffdd74351c15",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"43-analysis-3-sales-by-quarter-time-series\">4.3 Analysis 3: Sales by Quarter (Time Series)</h3>\n<span class=\"paragraph\">Visualization: Line chart</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "AjVT",
      "code_hash": "6e49338768d30899cf1242e074f3a689",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"task-50-product-and-geographic-analysis-visualizations\">Task 5.0: Product and Geographic Analysis Visualizations</h2>\n<span class=\"paragraph\">The following visualizations explore top-performing products, geographic patterns, and temporal shopping behavior to identify market drivers and consumer trends.</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "pHFh",
      "code_hash": "96968086790cdc1947fdb8b7d8b0be62",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"visualization-1-top-20-products-by-revenue\">Visualization 1: Top 20 Products by Revenue</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "aqbW",
      "code_hash": "bead0934b6dc251fb7234ccbf3f91b59",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"visualization-2-top-20-products-by-volume\">Visualization 2: Top 20 Products by Volume</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "TXez",
      "code_hash": "cc32ce600d0c2a75548a802419fdb723",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"visualization-3-top-15-counties-by-revenue\">Visualization 3: Top 15 Counties by Revenue</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "yCnT",
      "code_hash": "8052f9b4b3495474222f1cf59580c16e",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"visualization-4-top-20-cities-by-sales\">Visualization 4: Top 20 Cities by Sales</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "kqZH",
      "code_hash": "d202d811c31d5008a4e949fa3a5c8cf0",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"visualization-5-weekday-vs-weekend-sales-comparison-standardized\">Visualization 5: Weekday vs Weekend Sales Comparison (Standardized)</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "rEll",
      "code_hash": "1e95c8c27fb2997803e19e5ce51f5377",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"visualization-6-top-20-cities-by-sales-efficiency-average-sale-per-transaction\">Visualization 6: Top 20 Cities by Sales Efficiency (Average Sale per Transaction)</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "Hbol",
      "code_hash": "4c5491b7e33f366aa52db66f0de9298a",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "Data file not found!\nPlease download the dataset from:\nhttps://data.iowa.gov/Sales-Distribution/Iowa-Liquor-Sales/m3tr-qhgy\nAnd save it to: Iowa_Liquor_Sales.csv\n",
          "mimetype": "text/plain"
        }
      ]
    },
    {
      "id": "lEQa",
      "code_hash": "48f9ce6933b2bd65fba06745776b6bb9",
      "outputs": [],
      "console": []
    },
    {
      "id": "PKri",
      "code_hash": "871079c1e64c94edb5f932616db1b75e",
      "outputs": [],
      "console": []
    },
    {
      "id": "Xref",
      "code_hash": "ceda530500e8d9e54c2dae86903782a8",
      "outputs": [],
      "console": []
    },
    {
      "id": "RGSE",
      "code_hash": "35b64360ad610b8444be9fa3702d7528",
      "outputs": [],
      "console": []
    },
    {
      "id": "Kclp",
      "code_hash": "fa187d89743e6df366147828b3f3548b",
      "outputs": [],
      "console": []
    },
    {
      "id": "emfo",
      "code_hash": "4c1ddb9adeb04bd220248906950482b3",
      "outputs": [],
      "console": []
    },
    {
      "id": "nWHF",
      "code_hash": "4c38f8cf9273e44d0ad859ecefc1c97a",
      "outputs": [],
      "console": []
    },
    {
      "id": "iLit",
      "code_hash": "4e7037d6e1308777843b8ef909d115f5",
      "outputs": [],
      "console": []
    },
    {
      "id": "ZHCJ",
      "code_hash": "5d6c4e1e8759348439a52fb200ee08d5",
      "outputs": [],
      "console": []
    },
    {
      "id": "ROlb",
      "code_hash": "f6980c4c99200834781fa815fa69d3f6",
      "outputs": [],
      "console": []
    },
    {
      "id": "TqIu",
      "code_hash": "2fdbcb48a6ae0fe6c246e3fb7b1d0e5d",
      "outputs": [],
      "console": []
    },
    {
      "id": "DnEU",
      "code_hash": "c91dc2ec3271ec3420d69930b2d09f4f",
      "outputs": [],
      "console": []
    },
    {
      "id": "ulZA",
      "code_hash": "e3460aac76fb8511a6a5beea40a07774",
      "outputs": [],
      "console": []
    },
    {
      "id": "Pvdt",
      "code_hash": "5048949238f3d6c1a44ea793ec82d59e",
      "outputs": [],
      "console": []
    },
    {
      "id": "aLJB",
      "code_hash": "e42cffcd0a434a5d6a3ce9925dd7f36e",
      "outputs": [],
      "console": []
    },
    {
      "id": "xXTn",
      "code_hash": "1126f8ad323b76a381716846d5fbdd5f",
      "outputs": [],
      "console": []
    },
    {
      "id": "NCOB",
      "code_hash": "7968eebed1a593b89a9cf4b478a707fb",
      "outputs": [],
      "console": []
    },
    {
      "id": "TRpd",
      "code_hash": "b6fc9b5f8415fa7568b960391c2a6c89",
      "outputs": [],
      "console": []
    },
    {
      "id": "dNNg",
      "code_hash": "dc6cdd44721a1c4b4a30cfe2c8739de6",
      "outputs": [],
      "console": []
    },
    {
      "id": "wlCL",
      "code_hash": "a5f62f28ee52345c7ff17072e0e4fa06",
      "outputs": [],
      "console": []
    },
    {
      "id": "wAgl",
      "code_hash": "a74f618eb15b63dd27e94213d767c37a",
      "outputs": [],
      "console": []
    },
    {
      "id": "dGlV",
      "code_hash": "8260172cd331cd60e242855cac746828",
      "outputs": [],
      "console": []
    }
  ]
}